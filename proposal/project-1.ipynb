{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkQDuZm9pIiJ"
   },
   "source": [
    "## Project submission header\n",
    "### Submission preparation instructions \n",
    "_Completion of this header is mandatory, subject to point deduction to the scoping assignment._ Only add plain text in the designated areas, i.e., replacing the relevant 'NA's. You must fill out all group member Names and Drexel email addresses in the below markdown list, under header __Project group__. It is required to fill out descriptive notes pertaining to any __External support and stakeholders__, including project mentors (i.e., your advisors) or collaborative relationships in the completion of this submission under the __Additional submission comments__ section at the bottom of the header; please list any supplementary comments pertaining to the submissions in the section labeled __Other__. If no other support was received or parties are involved, leave NA for these sections. \n",
    "\n",
    "For these projects, the target group sizes should be 3&ndash;4 students, but large groups of 5&ndash;6 will be allowed with justification:\n",
    "> if you wish to conduct your project in a large group of more than 4, you must complete the __Large groups justification__ section, which should express why exactly your project will particularly benefit from the larger group, e.g., if you're all working together on a capstone project that you'd like to enrich with NLP+DL. Additionally, large groups are required to submit an additional statement on __Group workload management__, desc\n",
    "\n",
    "_Any distruption of this header's formatting will make your group liable to a possible point deduction._\n",
    "\n",
    "### Project group\n",
    "- Group member 1\n",
    "    - Name: Farzan Mirza\n",
    "    - Email: fm474@drexel.edu\n",
    "- Group member 2\n",
    "    - Name: Nakul Narang\n",
    "    - Email: nn474@drexel.edu\n",
    "- Group member 3\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "- Group member 4\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "- Group member 5\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "- Group member 6\n",
    "    - Name: NA\n",
    "    - Email: NA\n",
    "\n",
    "### Additional submission comments\n",
    "- External support and stakeholders: NA\n",
    "- Other (other): NA\n",
    "- Large groups justification: NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4roU5IlbvJb"
   },
   "source": [
    "# DSCI 691: Natural Language Processing with Deep Learning <br> Term Project Phase 1: Scoping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnB9M_yJbvJe"
   },
   "source": [
    "## Overview\n",
    "As this is an elective course, you all presumably know some idea of why you're here, i.e., you have some concept of what NLP and DL are and interest in working with building systems which rely on state-of-the-art language learning techniques.\n",
    "\n",
    "So this is&mdash;like for many other DSCI courses&mdash;the first portion of a two-part, open-ended team assignment. Term projects will account for 40% of your overall grade, and this scoping section is worth 10% (1/4 of the project). The other 3/4 will include project performance and culminate in a presentation during the last week of class or the regularly scheduled final exam period. As an upper-level elective course, this one is designed to support research and research-oriented and open-ended curricular requirements, like capstones, funded research work, theses, and dissertations.\n",
    "\n",
    "All projects for this course will entail the following two phases:\n",
    "\n",
    "1. (this) scoping phase, which must include the following main components:\n",
    "    - the review of a research paper relevant to your project and write-up of a summary and\n",
    "    - a NLP+DL project description; which will be followed by\n",
    "2. (the next) performance phase, covering exporation and prototyping of an application's function or of an empirical investigation, i.e., applying NLP+DL to illustrate a scientific reserach question.\n",
    "\n",
    "__Important__: Please fill out this Jupyter notebook to complete your project scoping and submit the completed copy with any supplementary materials attached. Specific requirements are listed below. _Each member of each must submit their own exact-same copy of the scoping document._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQqR8G2gbvJg"
   },
   "source": [
    "## 1. Project and team formation (1 pt)\n",
    "\n",
    "The first thing you'll have to do in this phase is organize into a project team. \n",
    "\n",
    "Be sure to consider the range of strengths in your group and their interests in NLP+DL, as it will help to discuss these in defining a project. Be sure to write out the names of the project team's members in this first report, below, and answer the two questions:\n",
    "\n",
    "1. What areas/skills/domains does the team member presently identify with?\n",
    "Ans) Language Modeling, Reinforcement Learning\n",
    "2. Into which areas/skills/domains would the team member like to grow?\n",
    "Ans) understanding GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2oC-6q0bvJh"
   },
   "source": [
    "### Project title, abstract, and expanded team description\n",
    "\n",
    "Here, you should provide and informative name (this can be updated later) for your project, in addition to a 150-word maximum project abstract, providing a short, intuitive description of the project's intent.\n",
    "- Title: GANs and Transformers in Abstractive Text Summarization\n",
    "- Abstract:\n",
    "    > This project explores the advancement of abstractive text summarization (ATS) through the application of a Generative Adversarial Network (GAN), enhancing the natural language processing (NLP) field. Building upon a foundational 2017 study, we employ a Transformer architecture to overcome the limitations of LSTM-based models cited in the original research, such as exposure bias and discrepancies between training loss and evaluation metrics. Our model integrates diverse datasets, including news articles, book plots, and product descriptions, aiming for robust generalizability across various text types. We enhance our model's learning efficiency and adaptability by utilizing ADAM optimization and evaluating its performance with ROUGE metrics. This project not only aims to surpass existing benchmarks in ATS but also explores the potential of GANs within NLP to produce contextually relevant and grammatically precise summaries, pushing the boundaries of machine-generated text summarization.\n",
    "\n",
    "Additionally, each team member must fill out the below information. \n",
    "\n",
    "- Group member 1\n",
    "    - Name: Farzan Mirza\n",
    "    - Interests: Artificial Intelligence, Computer Science, NBA, WWE, Gym, Reading, Movies\n",
    "    - Background: I was born in Kanpur, India. Moved to Singapore for highschool and completed my IB diploma. Went to Virginia Tech for my undergrad in Computational Modelling and Data Analytics then found a startup back in India to create an ERP for the manufacturing industry. A year later came to Drexel to pursue my masters in AIML. I have been coding since the age of 11, I initially started with web development learning HTML, PHP, CSS and MySQL before learning Java and C as part of the required schoolwork in highschool. In undergrad I became proficient with Python and R. While working on my startup I learnt how to setup AWS services like RDS, EC2, S3 Bucket and Route 53. I also learnt git specifically on gitlab first where I managed to setup CI/CD pipelines using NPM for Angular and Docker for Laravel code. I presently identify with all the skills/areas I mentioned previously and I think I am good at AI but that is an area I would like to grow in more specifically Fuzzy Logic\n",
    "- Group member 2\n",
    "    - Name: Nakul Narang\n",
    "    - Interests: Transformers, GANs\n",
    "    - Background: Language Modelling, Reinforcement Learning\n",
    "    \n",
    "### Group workload management \n",
    "Note: this section is only required for large groups of 5 or more, and is otherwise optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHaMtpuGbvJi"
   },
   "source": [
    "## 2. Research paper summary (4 pts)\n",
    "This first section of your proposal must summarize a relevant research to approximately 2 pages/1000 words in the markdown cell, below. Here, you're required to write a summary written at a level that your classmates will understand. Hence, and new terminology or technical details not covered in the scheduled course topics will need to be covered in detail, but you should avoid spending space on course-covered content. First, list the metadata for the research paper\n",
    "\n",
    "Note: please be critical of the literature you select and reach out to your instructor and/or TAs to ensure it is of high quality. If you select a paper that has not passed through peer review, please understand it may not be fully resolved or evaluated research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MIehqNebvJk"
   },
   "source": [
    "### Paper summary metadata\n",
    "Please place your paper's metadata in the labeled sections below.\n",
    "- Title: Generative Adversarial Network for Abstractive Text Summarization\n",
    "- Link:https://arxiv.org/abs/1711.09357\n",
    "- Bibliographical information: Liu, L., Lu, Y., Yang, M., Qu, Q., Zhu, J., & Li, H. (2018). Generative Adversarial Network for abstractive text summarization. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). https://doi.org/10.1609/aaai.v32i1.12141 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T36xMqOYbvJk"
   },
   "source": [
    "### Description of paper-review sections\n",
    "In your 2-page summary, you must complete the following sections:\n",
    "1. __Justification__: a justification for the review, which _must cover why_ this paper is relevant to your project;\n",
    "2. __Background__: a background of the paper, e.g., related work, research motivation, and placement in the history of NLP research; \n",
    "3. __Impact__: a summary of contributions, e.g., what the paper teaches us and how it has or may impact the NLP community; and finally\n",
    "4. __Discussion__: a discussion of the work and its limitations, e.g., where the work loses applicability or needs improvement or expansion and possible directions for future work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "splm3NxFbvJl"
   },
   "source": [
    "_Please fill out the below sections to complete your __Research paper summary__._\n",
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_m7FkyebvJl"
   },
   "source": [
    "#### 2.1 Justification\n",
    "\n",
    "This paper served as a foundation for our term project that aims to do abstractive text summarization (ATS), which involves generating new text that captures the essential meanings of the original, often using phrases and constructs not found in the source. It addresses the limitations of traditional sequence-to-sequence models, which often produce generic and grammatically limited summaries due to their reliance on maximum likelihood estimation (MLE) training methods. These methods introduce discrepancies between the training loss and evaluation metrics and lead to \"exposure bias,\" where the model overly depends on the ground truth during training and fails to generalize during testing. The following points highlight this study's relevance to our project:\n",
    "\n",
    "1. **Using General Adversarial Network (GAN):** An adversarial training setup is created in which the generative model (G) strives to deceive the discriminative model (D) into classifying machine-generated summaries as human-generated. This ensures the summaries are grammatically precise and contextually relevant as they become increasingly indistinguishable from human writing. Consequently, this study effectively implements a GAN for abstractive text summarization, helping us plan our approach.\n",
    "\n",
    "2. **Optimization Strategies for GAN:** The paper outlines the effective use of deep learning (DL) methodologies for natural language processing (NLP). It utilizes reinforcement learning, specifically the policy gradient algorithm, to optimize the generative model for producing highly rewarded summaries. Additionally, it proposes a minimax two-player game strategy to enhance the generative model (G) and the discriminative model (D), integrating AI concepts previously covered in our coursework. We plan to implement these strategies to optimize our adversarial setup similarly.\n",
    "\n",
    "3. **Generative Model Architecture**: The neural network architecture described for the generative model in the referenced paper has significantly influenced the design of our generative model. The paper provided a detailed rationale for the arrangement of the layers and the mathematical principles underlying them. Specifically, it employed a bi-directional LSTM encoder to capture comprehensive contextual details and an attention-based LSTM decoder to focus selectively on the most relevant text for the current word generation. This method of selective attention closely resembles human summarization techniques, which concentrate on essential information and articulate it succinctly. These insights motivated us to adopt the Transformer architecture in our project, recognizing its inherent capabilities for both encoding and decoding. The Transformer's self-attention mechanism, which processes the entire text sequence simultaneously rather than sequentially as in bi-directional LSTMs, eliminates the need for separate mechanisms to integrate forward and backward contexts. This paper clarified the encoding and decoding processes and demonstrated the use of a fully connected and softmax layer to predict the probability distribution of the next words in a summary. Inspired by this setup, we plan to create a similar neural network structure for our Transformer-based model.\n",
    "\n",
    "4. **Discriminative Model Architecture:** In refining the discriminative model described in the research paper, we made a strategic adjustment. The original model employs a CNN using Word2Vec for encoding, applies various filters for feature extraction, and uses max-over-time pooling to reduce dimensionality. These features are then processed through a fully connected layer and a softmax layer, which outputs the probability of the summary being human or machine-generated. We opted to replace the softmax layer with a logistic regression layer to streamline computations for our binary classification task, reducing unnecessary computational overhead by directly computing the probability for one class.\n",
    "\n",
    "5. **Model Parameter Update:** Inspired by the paper's use of stochastic gradient descent (SGD) for updating model parameters, we opted to use Adaptive Moment (ADAM) optimization instead. Given its efficiency with sparse gradients and adjustable learning rates for each parameter, it generally leads to faster convergence. \n",
    "\n",
    "6. **Diverse Datasets:** The paper was trained and tested exclusively on a dataset comprising CNN and Daily Mail news stories. We plan to use various data sources for texts and their human-generated summaries, anticipating that this approach will make our model adaptable to summarizing diverse bodies of text beyond just news articles.\n",
    "\n",
    "7. **Performance Evaluation:** The ROUGE-1, ROUGE-2, and ROUGE-L statistics utilized in the paper will also serve as our metrics to evaluate the performance of our GAN. ROUGE-1 measures the overlap of identical words between the reference text and the generated summary, assessing content similarity at the word level. ROUGE-2 examines the co-occurrence of two consecutive words, providing insights into the preservation of specific information and phrasing, crucial for evaluating grammatical and syntactical quality. ROUGE-L assesses the longest common sequence of words between the generated and reference summaries, indicating the fluency and structural coherence of the summary. We aim to surpass the ROUGE scores reported in the reviewed paper with our project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7hWzCR8bvJm"
   },
   "source": [
    "#### 2.2. Background\n",
    "\n",
    "This paper refers and builds upon the following related to conduct it’s study\n",
    "\n",
    "1. **(Nallapati et al. 2016):** To get the initial dataset and compare performance with the abstractive model ABS\n",
    "2. **(Goodfellow et al. 2014):** Information on how to construct the GAN\n",
    "3. **(Liu and Manning 2017):** Implementation of the generator model\n",
    "4. **(Kim 2014):** Idea for CNN encoder in discriminative model\n",
    "5. **(Sutton et al. 2000):** Logic for policy gradient algorithm\n",
    "6. **(Paulus, Xiong, and Socher 2017):** Compare performance with the abstractive deep reinforced model DeepRL\n",
    "\n",
    "In 2017, Vaswani et al.'s introduction of the Transformer, which significantly outperformed systems based on recurrent architectures like LSTMs, had yet to fully permeate the research community's applied methodologies. This paper, therefore, represents a crucial point in the timeline of NLP where traditional approaches were being challenged but not yet replaced. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vn9PSB45bvJp"
   },
   "source": [
    "#### 2.3 Impact\n",
    "\n",
    "The paper's innovative approach to abstractive text summarization using a GAN framework marks a significant contribution to the field of NLP. By integrating advanced techniques such as reinforcement learning and adversarial training, the study addresses and effectively mitigates common issues like exposure bias and the gap between training loss and evaluation metrics in sequence-to-sequence models. The successful application of these methodologies provides a robust foundation for future explorations and applications in automatic text summarization, particularly in enhancing the generated text's grammatical quality and contextual appropriateness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX_PKrUvbvJp"
   },
   "source": [
    "#### 2.4 Discussion\n",
    "\n",
    "While the paper sets a new precedent in abstractive summarization, there are areas where its applicability may be limited or require further enhancement. One notable limitation is the model's training and testing on a singular, news-focused dataset, which may only generalize across other domains or text types with further adaptation. This specificity might restrict the model's effectiveness in diverse applications, suggesting a potential direction for future work to explore broader dataset utilization and cross-domain efficacy. Additionally, the model's complexity and the need for substantial computational resources could pose challenges in deployment scenarios that demand low latency or limited hardware capabilities. Future research could focus on optimizing the model's efficiency or developing more lightweight versions suitable for such environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c-X1FwIbvJq"
   },
   "source": [
    "## 3. Full project description (5 pts)\n",
    "The full project description has six sections, covering the following (below). \n",
    "\n",
    "### Description of project description sections\n",
    "\n",
    "1. __Project goals__: the main goals of the project, including any high level information like scientific questions you're trying to answer, deployment scenarios your trying satisfy, or how this project's NLP pipeline fits into a larger application context.\n",
    "2. __NLP task description__: the NLP task you'll address, which should discuss the specific NLP task that your project will cover as it relates to others in the NLP research literature, or, if your task is non-standard, should develop a precise task description and place it _near_ others in the research literature.\n",
    "3. __Project data__: a desription which should address how the your project's task is exactly formulated in data, e.g., discussing the task information connects to some typed output and over what units, like tokens or spans or if/how it's related to generation.\n",
    "4. __Neural methodology__: a description of the neural method(s) that you intend to employ/explore towards the satisfaction of your project's goals and why this approach is appropriate for the specific task.\n",
    "5. __Baselines__: the baselines you'll use for comparison with your neural methodology, and which should illustrate what's 'easily' accomplished in your task by standard, conventional, rule- or frequency-based, naïve, and otherwise simplified but illustrative approaches to task satisfaction.\n",
    "6. __Evaluation__: an plan for how you will evaluatate the performance of your model against the baselines, including any expected experiental design criteria and approaches to exploration of hyperparameters, including, e.g., ablation studies and cross-validations etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0TRZbiYbvJq"
   },
   "source": [
    "_To complete your __Project description__, please fill out the below markdown sections._\n",
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6OzQqZDbvJr"
   },
   "source": [
    "#### 3.1 Project goals\n",
    "\n",
    "The main goal of this project is to advance the field of automatic text summarization using deep learning and NLP. Specifically, we aim to develop a system capable of generating abstractive text summaries indistinguishable from those written by humans. This involves addressing scientific questions about natural language understanding and generation, such as handling context, semantics, and coherence in generated text. The project seeks to enhance academic understanding and methodologies and explore potential deployment scenarios like summarizing content for news aggregation platforms, educational purposes, or customer reviews, thereby fitting into larger application ecosystems. We also aim to beat the ROUGE scores obtained in the paper “Generative Adversarial Network for Abstractive Text Summarization,” upon which our work is based. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1tB-qAZbvJs"
   },
   "source": [
    "#### 3.2 NLP task description\n",
    "\n",
    "The NLP task addressed by this project is abstractive text summarization, which involves generating new phrases and sentences that succinctly capture the essential meaning of the original text. Abstractive summarization requires the creation of novel text elements, mimicking how a human might summarize a complex text. This makes it a challenging sequence-to-sequence problem, where the input sequence (the original text) must be transformed into a new, shorter output sequence (the summary). The task demands a deep understanding of language and the ability to express core ideas in new ways, pushing the boundaries of machine understanding of natural language. It requires the grasp of factual details and nuances of context, tone, and intent, making it highly relevant for applications such as news aggregation, content creation, educational tools, and customer service, where effective summarization can add significant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaKok7k5bvJs"
   },
   "source": [
    "#### 3.3 Project data\n",
    "\n",
    "The CNN/DailyMail dataset, which we will utilize in this study, consists of human-generated abstractive summary bullets derived from news stories on the CNN and Daily Mail websites. These summaries are structured as questions (with one entity obscured) corresponding to stories that serve as passages from which the system must infer and fill in the blanks. The dataset includes tools for crawling, extracting, and generating passage and question pairs from these websites.\n",
    "\n",
    "The dataset has the following article \n",
    "\n",
    "- “By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 Octob…” \n",
    "\n",
    "This article is mapped to the summary \n",
    "\n",
    "- “Bishop John Folda, of North Dakota, is taking time off after being diagnosed . He contracted the inf…” \n",
    "\n",
    "The example article above is what will be passed to our generator for it to generate a summary, which will be compared with the summary in the dataset above in the discriminator. \n",
    "\n",
    "The provided scripts structure the dataset, which comprises 286,817 training pairs, 13,368 validation pairs, and 11,487 test pairs. On average, the source documents in the training set contain 766 words spread across 29.74 sentences, while the summaries are typically 53 words and 3.72 sentences long. The complete dataset is segmented into three primary divisions: training, validation, and testing, with an overall size of 1.37 GB. \n",
    "\n",
    "During the project development phase, contingent upon time availability, we aim to initially surpass the benchmarks established in the reviewed paper using the same dataset. Subsequently, we plan to enhance the dataset by incorporating a diverse array of text sources and their corresponding summaries to ensure the model's generalizability. For instance, we intend to include datasets comprising book plots, Reddit posts, and Amazon product descriptions, along with their summaries. This expansion will enrich the training data and enable the model to handle a wider variety of text types, thereby improving its applicability and robustness across different domains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYF2g2NqbvJt"
   },
   "source": [
    "#### 3.4 Neural methodology\n",
    "\n",
    "The generative component of the model will utilize a Transformer architecture for language modeling. Unlike traditional sequence-to-sequence models that employ bi-directional LSTMs, the Transformer model leverages self-attention mechanisms. This design allows each position in the decoder to attend to all positions in the input sequence simultaneously, thus efficiently managing long-range dependencies. This capability is crucial for generating coherent and contextually rich summaries from large texts. The self-attention mechanism also enables the model to scale effectively with increased data and complexity, which is anticipated to enhance the overall quality of the generated summaries.\n",
    "The generative model is a neural network structured so that the text is initially fed into the transformer, and the output from the transformer goes into a fully connected layer. Finally, the resultant output is fed into a softmax layer to predict the probability distribution of the next words in a summary.\n",
    "\n",
    "The discriminator in the adversarial setup will employ a convolutional neural network (CNN) that encodes the input sequences. This CNN will be built atop a Word2Vec embedding layer, which provides a dense representation of words based on their contexts in large datasets. The CNN will utilize multiple filters of varying sizes to extract a broad range of features from the text, which are then pooled using max-over-time pooling to capture the most significant features. These pooled features are then fed into a fully connected layer followed by a logistic regression layer. We opted to replace the softmax layer with a logistic regression layer to streamline computations for our binary classification task, reducing unnecessary computational overhead by directly computing the probability for one class.\n",
    "\n",
    "The training of the generator and discriminator will be conducted in an adversarial manner, following the minimax game theory approach. In this setup, the generator aims to produce summaries that the discriminator cannot easily distinguish from human-generated summaries, thereby driving the generator to improve iteratively. Meanwhile, the discriminator is trained to accurately classify the summaries as human or machine-generated, refining its ability to detect subtleties in the generated text. This method helps to address common issues in neural text generation, such as exposure bias and the discrepancy between training objectives and evaluation metrics.\n",
    "\n",
    "Reinforcement learning techniques, specifically policy gradients, will be used to enhance the generative model further. This approach allows the generator to receive feedback directly from the discriminator’s performance, which acts as a reward signal. This dynamic adjustment helps the generator focus on producing high-quality outputs that are more likely to be classified as human-like by the discriminator. It also enables the model to bypass the limitations associated with non-differentiable performance metrics typically used in text summarization tasks.\n",
    "\n",
    "Adaptive Moment (ADAM) optimization is used to update the model parameters given its efficiency with sparse gradients and adjustable learning rates for each parameter, it generally leads to faster convergence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCe8qefebvJt"
   },
   "source": [
    "#### 3.5 Baselines\n",
    "\n",
    "In our project, we are comparing the performance of our proposed model against several established baselines to assess its effectiveness in generating abstractive text summaries. These baselines include:\n",
    "\n",
    "- **ABS (Nallapati et al., 2016):** This represents one of the earlier approaches to abstractive text summarization, employing sequence-to-sequence RNNs. It is a fundamental comparison point to illustrate the advancements in handling the summarization task over time.\n",
    "- **Pointer-Generator Coverage Networks (PGC) (See, Liu, and Manning, 2017):** This model enhances the sequence-to-sequence framework by integrating a pointer-generator mechanism, which allows for the copying of words directly from the source text alongside generating new words from a fixed vocabulary. It also incorporates a coverage mechanism to prevent the issue of repetitive text generation, thus improving the diversity and relevance of the summaries.\n",
    "- **Abstractive Deep Reinforced Model (DeepRL) (Paulus, Xiong, and Socher, 2017):** This model combines traditional maximum likelihood estimation (MLE) training with reinforcement learning to optimize a mixed objective. This strategy aims to enhance both the summaries' relevance and fluency by addressing typical shortcomings associated with MLE, such as exposure bias.\n",
    "- **Generative Adversarial Network for Abstractive Text Summarization:** We are also studying a recent approach that utilizes a generative adversarial network (GAN) setup. In this model, the generator is designed to create difficult summaries for the discriminator to distinguish from human-generated text, promoting the generation of more abstractive, readable, and diverse summaries. This method is part of our literature review and inspires our model development.\n",
    "\n",
    "Using these baselines, we aim to demonstrate that our model adheres to the conventional expectations of summary generation and surpasses them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SSGzA3RbvJw"
   },
   "source": [
    "#### 3.6 Evaluation\n",
    "We will use ROUGE-1, ROUGE-2, and ROUGE-L statistics as metrics to evaluate the performance of our GAN. ROUGE-1 measures the overlap of identical words between the reference text and the generated summary, assessing content similarity at the word level. ROUGE-2 examines the co-occurrence of two consecutive words, providing insights into the preservation of specific information and phrasing, crucial for evaluating grammatical and syntactical quality. ROUGE-L assesses the longest common sequence of words between the generated and reference summaries, indicating the fluency and structural coherence of the summary."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project-1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
